一般来说，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数，多元分类问题可以使用 softmax函数
## Sigmoid 函数
$$
h(x)=\frac{1}{1+exp(-x)}
$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20241015232046.png" style="zoom:70%">

## ReLU 函数
rectified linear unit
$$
ReLU(x) = max(x,0)
$$
$$
h(x)=\left\{
\begin{aligned}
x~ ~(x > 0) \\
0~ ~(x \leq 0)\\
\end{aligned}
\right.
$$
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20241015232039.png" style="zoom:50%">
## softmax
softmax 函数的输出在 0.0 ~ 1.0 之间。
并且它的函数输出值总和是 1。
$$
y_k = \frac{exp(a_k)}{\sum_{i=1}^nexp(a_i)}
$$
$n$ 是输出层神经元的个数，第 $k$ 个神经元的输出是 $y_k$。输出层的各个神经元都受到所有输入信号的影响。
![](https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20241015234530.png)
由于当指数特别大时，可能会造成溢出。可以采用如下方法对 softmax 函数进行改进
![](https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20241015234800.png)
C’ 一般使用输入信号中的最大值。
## Tanh
将输入投影到 (-1, 1)
$$
tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}
$$
