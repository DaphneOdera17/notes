# 机器学习 | 决策树
## 熵 Entropy
表示随机变量的不确定性
$H(p)=-\sum_{i=1}^np_ilogp_i$
熵越大，随机变量的不确定性越大。反之，不确定性越小。
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240831110518.png" style="zoom:70%">
### 计算数据集的熵
$H(D)=-\sum_{k=1}^k\frac{|C_k|}{D}log_2\frac{|C_k|}{D}$
$D$ 表示数据集的条数，$k$ 表示数据集的分类有几个类别，$C_k$ 表示这个类别有几个。
## 条件熵
$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$，其中 $p_i=P(X=x_i)$
$$
\begin{aligned}
H(D|A)&=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{i=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}
\end{aligned}
$$
对于给定表格：

| ID  | 年龄  | 有工作 | 类别  |
| --- | --- | --- | --- |
| 1   | 青年  | 否   | 否   |
| 2   | 青年  | 否   | 否   |
| 3   | 青年  | 是   | 是   |
| 4   | 青年  | 是   | 是   |
| 5   | 中年  | 否   | 否   |
| 6   | 中年  | 否   | 否   |
| 7   | 中年  | 否   | 否   |
| 8   | 中年  | 否   | 否   |
| 9   | 中年  | 是   | 是   |
| 10  | 中年  | 否   | 是   |
| 11  | 老年  | 否   | 是   |
设 A 为年龄，则 n = 3，
设 $D_1$ 为青年，$D_2$ 为中年, $D_3$ 为老年。$k=1$ 为否，$k=2$ 为是。
$$
\begin{aligned}
H(D|A)&=-(\frac{|D_1|}{|D|}H(D_1)+\frac{|D_2|}{|D|}H(D_2)+\frac{|D_3|}{|D|}H(D_3))\\
&=-(\frac{4}{11}(\frac{2}{4}log_2\frac{2}{4}+\frac{2}{4}log_2\frac{2}{4})+\frac{6}{11}(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})+\frac{1}{11}(\frac{0}{1}log_2\frac{0}{1}+\frac{1}{1}log_2\frac{1}{1}))
\end{aligned}
$$
## 信息增益
$g(D,A)=H(D)-H(D|A)$，其中 $D$ 是训练数据集，$A$ 是某个特征。
根据信息增益的特征选择方法是：
	1.对训练数据集或子集 D，计算每个特征的信息增益。
	2.比较他们的大小，选择信息增益最大的特征。
### 信息增益算法
#### 1.计算数据集 D 的经验熵 H(D)
$H(D)=-\sum_{k=1}^k\frac{|C_k|}{D}log_2\frac{|C_k|}{D}$
#### 2.计算特征 A 对数据集 D 的经验条件熵 H(D|A)
$H(D|A)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{i=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$
#### 3.计算信息增益
$g(D,A)=H(D)-H(D|A)$
## ID3 算法构建决策树
在决策树递归构建过程中，使用信息增益的方法进行特征选择
### 决策树生成过程
	1.从根节点开始计算所有特征的信息收益，选择信息收益最大的特征作为节点特征
	2.再对子节点递归调用以上方法，构建决策树
	3.所有特征信息增益很小或者没有特征可以选择时递归结束得到一颗决策树
