# 机器学习 | 梯度下降算法
## 梯度
梯度的方向是函数值上升最快的方向，梯度的反方向就是函数值下降最快的方向。
对于 $J(\theta)=\theta^2+2\theta+5$, $\nabla J(\theta)=2\theta+2$
想求 $J(\theta)$ 的最小值，那我们就沿下降的方向，给定步长 $\alpha$，假设 $\alpha=0.06$，则减小一次，
$$
\begin{aligned}
\theta &= 3-0.06\times(\nabla J(\theta)) \\
&=3-0.06\times(2\times3+2)=2.52
\end{aligned}
$$
不断进行迭代，那么最终 $\theta$ 就会在最小值附近。
## 梯度下降法 Gradient Descent
随机初始化 $\theta$
设置步长 $\alpha$，设置迭代次数 $m$
求 $J(\theta)$ 的导数 $\nabla J(\theta)$
```
for i = 0 to m:
	θ := θ - α∇J(θ)
```
如果 $J(\theta)$ 有多个变量，例如
$J(\theta)=\theta_1^2+\theta_2+\theta_3$，则依次求偏导，$\nabla J(\theta_1)=2\theta_1,\nabla J(\theta_2)=1,\nabla J(\theta_3)=1$
```
for i = 0 to m:
	θ1 := θ1 - α∇J(θ1)
	θ2 := θ2 - α∇J(θ2)
	θ3 := θ3 - α∇J(θ3)
```
## 代码实现
```python
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(-6, 4, 100)
y = x ** 2 + 2 * x + 5
plt.plot(x, y)
```
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240830144106.png" style="zoom:50%">
```python
x = 3  # 随机初始化 x 值
alpha = 0.8   # 设置步长
iteraterNum = 100   # 设置迭代次数
for i in range(iteraterNum):
    x = x - alpha * (2 * x + 2)
print(x)  # -1.0
```
## 梯度下降法求解线性回归
使用梯度下降法求解，使得代价函数损失最小
$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial \theta_j}\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\\&=2\times\frac{1}{2m}\sum_{i=1}^m[(h_{\theta}(x^{(i)})-y^{(i)})\frac{\partial}{\partial \theta_j}(h_{\theta}(x^{(i)})-y^{(i)})]\\&=\frac{1}{m}\sum_{i=1}^{m}[(h_{\theta}(x^{(i)})-y^{(i)})\frac{\partial}{\partial\theta_j}(\sum_{f=0}^n\theta_fx_f^{(i)}-y^{(i)})]\\\nabla J(\theta_j)&=\frac{1}{m}\sum_{i=1}^m[(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}]\\\theta_j:&=\theta_j+\alpha\frac{1}{m}\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}
\end{aligned}
$$
## 梯度下降算法的不同变形
### 批量梯度下降 Batch Gradient Descent, BGD
$\theta_j:=\theta_j+\alpha\frac{1}{m}\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$
### 随机梯度下降 Stochastic Gradient Descent, SGD
$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$
### 小批量梯度下降 Mini-Batch Gradient Descent, MBGD
每次使用 $batch\_size$ 个样本进行更新
$\theta_j:=\theta_j+\frac{\alpha}{batch-num}\sum_{i=1}^{batch-num}(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$
推荐使用