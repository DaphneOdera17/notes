# 机器学习 | 正则化
## L2范数正则化解决过拟合 Ridge Regression，岭回归
### 岭回归求解
#### 目标函数
$$
\begin{aligned}
J(\theta)&=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda||\theta||_2^2 \\
&=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2~~~~(\lambda>0)
\end{aligned}
$$
#### 岭回归求解
$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2\\
&=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})+2\lambda\theta_j
\end{aligned}
$$
#### 迭代公式
$\theta_j:=\theta_j+\alpha\frac{1}{m}\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}-2\lambda\theta_j$

如果 $\lambda$ 越大，那么对模型的惩罚也就越大，系数就会变小，模型就变得简单。
## L1范数正则化解决过拟合 LASSO, 回归
$$
\begin{aligned}
J(\theta)&=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda|\theta|_1 \\
&=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n|\theta_j|~~~~(\lambda>0)
\end{aligned}
$$
## L1与L2结合解决过拟合 Elastic Net, 弹性网
$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda(\rho|\theta|_1+(1-\rho)||\theta||_2^2)~~ ~~(\lambda>0,0\leq\rho\leq1)$

