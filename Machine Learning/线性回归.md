# 机器学习 | 线性回归模型
也就是拟合一条直线，类似于 $y=ax+b$
## 符号约定
对于下面的表格

|           | $X_1$ | $X_2$ | $X_3$ | $X_4$ | ... | $X_n$ | $y$ |
| --------- | ----- | ----- | ----- | ----- | --- | ----- | --- |
| $X^{(1)}$ | 100   | 1     | 1990  | 1     | ... | ...   | 10  |
| $X^{(2)}$ | 105   | 2     | 2000  | 2     | ... | ...   | 15  |
| $X^{(3)}$ |       |       |       |       |     |       |     |
| $X^{(4)}$ |       |       |       |       |     |       |     |
| ...       |       |       |       |       |     |       |     |
| $X^{(n)}$ |       |       |       |       |     |       |     |
我们规定:
$X^{(1)}=[100,1,1990,1]$, $y^{(1)}=10$
$X^{(2)}=[105,2,2000,2]$, $y^{(2)}=15$
$$
\begin{aligned}
X_1&=
\begin{bmatrix}
100 \\
105 \\
\end{bmatrix}
\\
X_2&=
\begin{bmatrix}
1 \\
2 \\
\end{bmatrix}
\end{aligned}
$$
$X_3^{(2)}=2000$
## 形式化定义
### 假设函数 hypotheses function
相当于将 $b$ 对应于 $\theta_0x_0$，将 $ax$ 对应于 $\theta_1x_1$
$$
h(x)=\theta_0x_0+\theta_1x_1=\sum_{i=0}^1\theta_ix_i,~~其中~~x_0=1
$$
类似的，
$$
\begin{aligned}
h_{\theta}(x)&=\theta_0x_0+\theta_1x_1+...+\theta_nx_n\\&=\sum_{i=0}^n\theta_ix_i\\&=\theta^Tx~~(x_0=1)\\\vec{\theta}^T&=[\theta_0,\theta_1,...,\theta_n],\\\vec{x}&=\begin{bmatrix}x0\\x1\\...\\x_n\end{bmatrix}\end{aligned}
$$
### 损失函数 loss function
$L(\theta)=(h_{\theta}(x)-y)^2$，预测值和实际值差值的平方。
### 代价函数 cost function
$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)}))^2$
## 代码实现
```python
import numpy as np
import matplotlib.pyplot as plt
```
### 定义加载数据的函数
```python
def loaddata():
    data = np.loadtxt('data/data1.txt',delimiter=',')
    n = data.shape[1]-1 # 特征数, shape[1]获取列数, shape[0]获取行数, 特征数要去掉最后一列 y
    X = data[:,0:n]
    y = data[:,-1].reshape(-1,1)
    return X,y
```
### 特征归一化 Normalization
[详解特征归一化](https://blog.csdn.net/ybdesire/article/details/56027408)
用于消除数据特征之间的量纲影响。
令均值为 $u$，方差为 $\sigma$
则归一化后：$x=\frac{x-u}{\sigma}$
$\sigma=\sqrt{\frac{\sum_{i=1}^m(x^{(i)}-\overline{x})}{n-1}}$，除以 $n-1$ 是一个无偏估计，在代码中设置 $ddof=1$，如果未设置则是除以 $n$
[彻底理解样本方差为何除以n-1](https://blog.csdn.net/Hearthougan/article/details/77859173)
```python
def featureNormalize(X):
    mu = np.average(X,axis=0)
    sigma = np.std(X,axis=0,ddof=1)
    X = (X-mu)/sigma
    return X,mu,sigma
```
### 计算损失函数
```python
def computeCost(X,y,theta):
    m = X.shape[0]
    return np.sum(np.power(np.dot(X,theta)-y,2))/(2*m)
```
### 梯度下降法求解
```python
def gradientDescent(X,y,theta,iterations,alpha):
    c = np.ones(X.shape[0]).transpose() # 形成 X.shape[0] 行 1,并转置成列向量
    X = np.insert(X,0,values=c,axis=1)  # 对原始数据加入一个全为1的列, 插入到最前面, 相当于 X0
    m = X.shape[0] # 行数
    n = X.shape[1] # 特征数
    costs=np.zeros(iterations) 
    for num in range(iterations):
        for j in range(n):
            theta[j] = theta[j]+(alpha/m)*np.sum((y-np.dot(X,theta))*X[:,j].reshape(-1,1))
        costs[num] = computeCost(X,y,theta)
    return theta,costs
```
对于
```python
theta[j] = theta[j]+(alpha/m)*np.sum((y-np.dot(X,theta))*X[:,j].reshape(-1,1))
```
也就是 $\theta_j:=\theta_j+\alpha\frac{1}{m}\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}$
而 $h_{\theta}(x^{(i)})=\vec{\theta}^Tx$，这里直接用 $X\cdot\theta$
相当于
$$
\begin{aligned}X&=
\begin{bmatrix}
x_0^{(1)} & x_1^{(1)} & x_2^{(1)} \\
x_0^{(2)} & x_1^{(2)} & x_2^{(2)} \\
... & ... & ... \\
x_0^{(m)} & x_1^{(m)} & x_2^{(m)} \\
\end{bmatrix}
\\
\theta&=
\begin{bmatrix}
\theta_0\\
\theta_1\\
...\\
\theta_n\\
\end{bmatrix},~~~
y=\begin{bmatrix}
y^{(1)}\\
y^{(2)}\\
...\\
y^{(m)}
\end{bmatrix}\\
y&=X\cdot\theta
\end{aligned}
$$
#### 预测函数
```python
def predict(X):
    X = (X-mu)/sigma
    c = np.ones(X.shape[0]).transpose()
    X = np.insert(X,0,values=c,axis=1)
    return np.dot(X,theta)
```

```python
X,y = loaddata()
X,mu,sigma = featureNormalize(X)
theta = np.zeros(X.shape[1]+1).reshape(-1,1)
iterations = 400
alpha = 0.01
theta,costs = gradientDescent(X,y,theta,iterations,alpha)
# theta = gradientDescent(X,y,theta,iterations,alpha)
```
### 画损失函数图
```python
x_axis = np.linspace(1,iterations,iterations)
plt.plot(x_axis,costs[0:iterations])

print(theta)
```
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240830163420.png" style="zoom:50%">
### 画数据散点图和求得的直线
```python
plt.scatter(X,y)
h_theta = theta[0]+theta[1]*X
plt.plot(X,h_theta)
```
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240830163624.png" style="zoom:50%">
### 预测数据
```python
print(predict([[8.4084]]))
```
![](https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240830163702.png)
预测结果与数据集的进行比较，误差大概为 1
<img src="https://typora-birdy.oss-cn-guangzhou.aliyuncs.com/20240830163733.png" style="zoom:50%">
