# 机器学习 | 逻辑回归
## 逻辑回归 Logistic Regression
解决的是二分类的问题，类别分别用 0 和 1 表示。
### 假设函数
$$
\begin{aligned}
\hat y&=h_{\theta}(x)\\
&=g(\theta^Tx)\\
&=\frac{1}{1+e^{-\theta^Tx}}
\end{aligned}
$$
其中 $g(z)=\frac{1}{1+e^{-z}}$ 也就是 sigmoid 函数
### 损失函数
$L(\hat y,y)=-ylog\hat y-(1-y)log(1-\hat y)$
### 代价函数
$$
\begin{aligned}
J(\theta)&=\frac{1}{m}\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})\\
&=\frac{1}{m}\sum_{i=1}^m[-y^{(i)}log\hat y^{(i)}-(1-y^{(i)})log(1-\hat y^{(i)})]\\
&=\frac{1}{m}\sum_{i=1}^m[-y^{(i)}log\frac{1}{1+e^{-\theta^Tx}}-(1-y^{(i)})log(1-\frac{1}{1+e^{-\theta^Tx}})]
\end{aligned}
$$
### 逻辑回归求解
$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta)&=\frac{\partial}{\partial \theta_j}\frac{1}{m}\sum_{i=1}^m[-y^{(i)}log\frac{1}{1+e^{-\theta^Tx^{(i)}}}-(1-y^{(i)})log(1-\frac{1}{1+e^{-\theta^Tx^{(i)}}})]\\
&=\frac{\partial}{\partial \theta_j}\frac{1}{m}\sum_{i=1}^m[y^{(i)}log(1+e^{-\theta^Tx^{(i)}})+(1-y^{(i)})log(1+e^{\theta^Tx^{(i)}})]\\
&=\frac{1}{m}\sum_{i=1}^m[y^{(i)}\frac{-x^{(i)}_je^{-\theta^T}x^{(i)}}{1+e^{-\theta^Tx^{(i)}}}+(1-y^{(i)})\frac{x_j^{(i)}e^{\theta^Tx^{(i)}}}{1+e^{\theta^Tx^{(i)}}}]\\
&=\frac{1}{m}\sum_{i=1}^m[(-y^{(i)}+\frac{1}{1+e^{-\theta^Tx^{(i)}}})x_j^{(i)}]\\&=\frac{1}{m}\sum_{i=1}^m[(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}]
\end{aligned}
$$
#### 迭代公式
$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m[(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}]$
其中: $h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}$

## Sklearn
$\frac{1}{\lambda}=C$
C 越大，惩罚越小，易过拟合，泛化能力差。
C 越小，惩罚越大，不易过拟合，泛化能力好。
max_iter: 迭代次数
```python
model = linear_model.LogisticRegression(C=50, max_iter=2000)
model.fit(X, y)
```